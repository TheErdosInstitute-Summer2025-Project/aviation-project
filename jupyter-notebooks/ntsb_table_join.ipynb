{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3cd7e9",
   "metadata": {},
   "source": [
    "# NTSB Table Join\n",
    "\n",
    "This script takes separate tables from the NTSB data (2008-2025) and joins them together while respecting the aircraft-level unit analysis found in the supplemental tables. To do this, we construct a \"event_key\" variable which can be understood as a measure of a single aircraft in an event of some nature, rather than a measure at the event-level itself.  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91f639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd4f35",
   "metadata": {},
   "source": [
    "### \"Engines\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c893b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = pd.read_csv('../data/ntsb/ntsb_engines.csv',usecols=['ev_id',\n",
    "                                                            'Aircraft_Key',\n",
    "                                                            'eng_type',\n",
    "                                                            'eng_no']) # Extract relevant columns\n",
    "\n",
    "engines = engines[engines['eng_no']==1].drop(columns='eng_no') # Record information about first engine only\n",
    "engines['event_key'] = engines['ev_id'].astype(str) + '_' + engines['Aircraft_Key'].astype(str) # Create event_key "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e1151",
   "metadata": {},
   "source": [
    "# \"Aircraft\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded177ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_8123/3843993726.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../data/ntsb/ntsb_aircraft.csv',usecols=['ev_id',\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/ntsb/ntsb_aircraft.csv',usecols=['ev_id',\n",
    "                                                             'Aircraft_Key',\n",
    "                                                             'far_part',\n",
    "                                                             'damage',\n",
    "                                                             # 'acft_fire',   # sparse data, unlikely target, data leakage as feature\n",
    "                                                             # 'acft_expl',   # same as above\n",
    "                                                             'acft_make',\n",
    "                                                             'acft_category',\n",
    "                                                             'homebuilt',\n",
    "                                                             'total_seats',\n",
    "                                                             'num_eng',\n",
    "                                                             'fixed_retractable',\n",
    "                                                             'date_last_insp',\n",
    "                                                             'owner_acft',\n",
    "                                                             'certs_held',    \n",
    "                                                             'oprtng_cert',\n",
    "                                                             'oper_cert',\n",
    "                                                             # 'type_fly',    # unlikely to be useful\n",
    "                                                             'second_pilot',\n",
    "                                                             'evacuation',\n",
    "                                                             'rwy_len',\n",
    "                                                             'rwy_width',\n",
    "                                                             'acft_year',\n",
    "                                                             'fuel_on_board',\n",
    "                                                             'unmanned']    \n",
    "                                                             ) # Extract Relevant Columns\n",
    "\n",
    "\n",
    "data['event_key'] = data['ev_id'].astype(str) + '_' + data['Aircraft_Key'].astype(str) # Event key \n",
    "aircraft = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90828e44",
   "metadata": {},
   "source": [
    "# \"Findings\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5404779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCommenting out because of concern for data leakage\\n\\ndf = pd.read_csv(\\'../data/ntsb/ntsb_findings.csv\\',usecols=[\\'ev_id\\',\\n                                                             \\'Aircraft_Key\\',\\n                                                             \\'finding_description\\',\\n                                                             \\'Cause_Factor\\']) # Extract Relevant Columns\\n\\nindexCorF = df[ (df[\\'Cause_Factor\\'] != \\'C\\') & (df[\\'Cause_Factor\\'] != \\'F\\') ].index # Remove obs. that are not causes or cause factors\\ndf.drop(indexCorF , inplace=True)\\n\\ndf[\\'event_key\\'] = df[\\'ev_id\\'].astype(str) + \\'_\\' + df[\\'Aircraft_Key\\'].astype(str) # Generate Event Key\\n\\ndf = df[~df.finding_description.str.startswith(\\'Not determined\\')] # Remove \"not determined\" cause factors\\n\\ndf[\\'finding_description_category\\'] = df.finding_description.str.split(\\'-\\').str[0] # Separate broader CF category \\ndf.drop(df[df[\\'finding_description_category\\'] == \\'main system\\'].index, inplace = True) # Remove main system\\nfindings_dummies = pd.get_dummies(df[\\'finding_description_category\\'],dtype=int) # Dummy out Categories \\nnew_df = pd.concat([df,findings_dummies],axis=1).drop(columns=[\\'finding_description_category\\',\\n                                                               \\'Cause_Factor\\',\\n                                                               \\'ev_id\\',\\n                                                               \\'Aircraft_Key\\'])\\nnew_df\\nnew_df = new_df.groupby(new_df[\\'event_key\\']).sum()\\nfindings = new_df.reset_index()\\nfindings[\\'Aircraft_Key\\']=findings[\"event_key\"].str[-1].astype(int)\\nfindings[\\'ev_id\\'] = findings[\"event_key\"].str[:-2]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Commenting out because of concern for data leakage\n",
    "\n",
    "df = pd.read_csv('../data/ntsb/ntsb_findings.csv',usecols=['ev_id',\n",
    "                                                             'Aircraft_Key',\n",
    "                                                             'finding_description',\n",
    "                                                             'Cause_Factor']) # Extract Relevant Columns\n",
    "\n",
    "indexCorF = df[ (df['Cause_Factor'] != 'C') & (df['Cause_Factor'] != 'F') ].index # Remove obs. that are not causes or cause factors\n",
    "df.drop(indexCorF , inplace=True)\n",
    "\n",
    "df['event_key'] = df['ev_id'].astype(str) + '_' + df['Aircraft_Key'].astype(str) # Generate Event Key\n",
    "\n",
    "df = df[~df.finding_description.str.startswith('Not determined')] # Remove \"not determined\" cause factors\n",
    "\n",
    "df['finding_description_category'] = df.finding_description.str.split('-').str[0] # Separate broader CF category \n",
    "df.drop(df[df['finding_description_category'] == 'main system'].index, inplace = True) # Remove main system\n",
    "findings_dummies = pd.get_dummies(df['finding_description_category'],dtype=int) # Dummy out Categories \n",
    "new_df = pd.concat([df,findings_dummies],axis=1).drop(columns=['finding_description_category',\n",
    "                                                               'Cause_Factor',\n",
    "                                                               'ev_id',\n",
    "                                                               'Aircraft_Key'])\n",
    "new_df\n",
    "new_df = new_df.groupby(new_df['event_key']).sum()\n",
    "findings = new_df.reset_index()\n",
    "findings['Aircraft_Key']=findings[\"event_key\"].str[-1].astype(int)\n",
    "findings['ev_id'] = findings[\"event_key\"].str[:-2]\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a5ba298",
   "metadata": {},
   "source": [
    "# \"Injuries\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef5944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_8123/1748587254.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/ntsb/ntsb_injuries.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SERS', 'TOTL', 'MINR', 'FATL', 'NONE']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ntsb/ntsb_injuries.csv')\n",
    "\n",
    "## Remove NaN values to ensure valid summation when grouping by 'event_key'\n",
    "df = df[~df['inj_person_count'].isnull()]\n",
    "\n",
    "## Set new id for each case\n",
    "df['event_key'] = df['ev_id'].astype(str) + '_' + df['Aircraft_Key'].astype(str)\n",
    "\n",
    "## All possible injury levels\n",
    "print(list(set(df['injury_level'].values)))\n",
    "injury_levels = ['NONE', 'MINR', 'FATL', 'SERS']\n",
    "injured_levels = ['MINR', 'FATL', 'SERS']\n",
    "\n",
    "## Count the number of people at each injury level and the total number of injured individuals.\n",
    "\n",
    "injury_count_df = (\n",
    "    df[df['injury_level'].isin(injury_levels)]\n",
    "    .pivot_table(index='event_key', \n",
    "                 columns='injury_level', \n",
    "                 values='inj_person_count', \n",
    "                 aggfunc='sum', \n",
    "                 fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "## Rename columns\n",
    "injury_count_df.columns.name = None\n",
    "injury_count_df = injury_count_df.rename(columns={\n",
    "    'FATL': 'acft_fatal_count',\n",
    "    'SERS': 'acft_serious_count',\n",
    "    'MINR': 'acft_minor_count',\n",
    "    'NONE': 'acft_none_count'\n",
    "})\n",
    "\n",
    "## Add total and injured counts\n",
    "injury_count_df['acft_total_person_count'] = (\n",
    "    injury_count_df[['acft_fatal_count', 'acft_serious_count', 'acft_minor_count', 'acft_none_count']].sum(axis=1)\n",
    ")\n",
    "injury_count_df['acft_injured_person_count'] = (\n",
    "    injury_count_df[['acft_fatal_count', 'acft_serious_count', 'acft_minor_count']].sum(axis=1)\n",
    ")\n",
    "\n",
    "injury_count_df['ev_id'] = injury_count_df['event_key'].str.split('_').str[0]\n",
    "injury_count_df['Aircraft_Key'] = injury_count_df['event_key'].str.split('_').str[1].astype(int)\n",
    "injuries = injury_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08107005",
   "metadata": {},
   "source": [
    "# \"Event\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80b90e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_8123/3303626296.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../data/ntsb/ntsb_events.csv',usecols=['ev_id',\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/ntsb/ntsb_events.csv',usecols=['ev_id',\n",
    "                                                            'ntsb_no', # for exploratory purposes; makes it easier to find docket on NTSB website\n",
    "                                                            'ev_country',\n",
    "                                                            'ev_type',\n",
    "                                                            'ev_highest_injury',\n",
    "                                                            'inj_f_grnd',\n",
    "                                                            'inj_m_grnd',\n",
    "                                                            'inj_s_grnd',\n",
    "                                                            'inj_tot_f',\n",
    "                                                            'inj_tot_m',\n",
    "                                                            'inj_tot_n',\n",
    "                                                            'inj_tot_s',\n",
    "                                                            'inj_tot_t',\n",
    "                                                            'ev_time',\n",
    "                                                            'ev_year',\n",
    "                                                            'ev_month',\n",
    "                                                            'on_ground_collision',\n",
    "                                                            'latitude',\n",
    "                                                            'longitude',\n",
    "                                                            'apt_dist',\n",
    "                                                            'light_cond',\n",
    "                                                            'wx_dew_pt',\n",
    "                                                            'wind_vel_kts',\n",
    "                                                            'gust_kts',\n",
    "                                                            'altimeter',\n",
    "                                                            ]) # Select variables - this is our primary \"data\" object to which we will concatenate all  other sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71296387",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = {\n",
    "    'inj_tot_f': 'ev_inj_tot_f',\n",
    "    'inj_tot_m': 'ev_inj_tot_m',\n",
    "    'inj_tot_n': 'ev_inj_tot_n',\n",
    "    'inj_tot_s': 'ev_inj_tot_s',\n",
    "    'inj_tot_t': 'ev_inj_tot_t',\n",
    "}\n",
    "\n",
    "data.rename(columns=new_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8650c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = pd.read_csv('../data/ntsb/ntsb_from_carol.csv', usecols=['NtsbNo','BroadPhaseofFlight'])\n",
    "phase.rename({'NtsbNo':'ntsb_no'}, axis=1, inplace=True)\n",
    "\n",
    "data = pd.merge(data, phase, how='left', on=['ntsb_no'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb5d58",
   "metadata": {},
   "source": [
    "# Joining Event Data with Aircraft-Specific Data\n",
    "To join the primary \"event\" dataset, which is event-specific with the other tables which are aircraft-specific, we need to take a careful approach to how we go about joining them.  The general idea goes as follows:\n",
    "1. Create the \"event_key\" variable in the aircraft-specific datasets, which takes the format '{ev_id}_{Aircraft_Key}'.\n",
    "2. Join the lower-level datasets together to maximize the number of observations.  Some will likely be in one and not another, but what is important is that we collect a list of all individual aircraft-level observations.\n",
    "3. Once all lower-level datasets have been joined together and we have a list of all events with multiple aircraft, we can export a \"aircraft_count\" variable which expresses the number of \"Aircraft_Key\" for every \"ev_id.\"  \n",
    "4. Join this \"aircraft_count\" column into the \"Event\" dataset - now we have a count of how many planes were involved in each event.\n",
    "5. Create a function which duplicates every row in \"Events\" (aircraft_count - 1 times).  Thus, if there's 3 planes, we'll get 2 new rows of the event.\n",
    "6. Re-create the \"Aircraft\" variable with a groupby() and cum_count() function, so that every row per ev_id is added to until there are no more observations left (will be clearer in the code).\n",
    "7. Now that we have the dataset formatted to resemble the individual-aircraft-level data from other tables, we can create the \"event_key\" - our master joining variable - in the events data.\n",
    "8. Join all datasets on the Events data by \"event_key\", \"Aircraft_Key\", and \"ev_id\" to ensure we are joining the right aircraft/event combos onto the event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "099e17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.merge(engines,aircraft,on=['event_key','ev_id','Aircraft_Key'],how='left') # Join on all 3 to avoid _x and _y duplicate columns.  Also ensures correct specification.\n",
    "#tables = pd.merge(tables,findings,on=['event_key','ev_id','Aircraft_Key'],how='left') # Join \"findings\" data as well. Commented out because of possible data leakage\n",
    "tables = pd.merge(tables,injuries,on=['event_key','ev_id','Aircraft_Key'],how='left') # Join \"injuries\" data as well.\n",
    "tables['Aircraft_ID'] = tables.groupby('ev_id').cumcount() + 1 # Some events start at 2 - this line creates a new Aircraft Key that is uniform across all coding schemes.\n",
    "tables['event_key'] = tables['ev_id'].astype(str) + '_' + tables['Aircraft_ID'].astype(str) # Resets the \"event_key\" variable to match our adjusted aircraft ID\n",
    "\n",
    "aircraft_counts = pd.DataFrame(tables.groupby('ev_id')['Aircraft_ID'].count()).reset_index() # Counts how many unique values of \"Aircraft_ID\" per event - need this to tell \"events\" set where to duplicate\n",
    "aircraft_counts.rename(columns={'Aircraft_ID':'aircraft_count'},inplace=True)\n",
    "aircraft_counts['aircraft_count'] = aircraft_counts['aircraft_count'].fillna(1) # Fill in missing - if no aircraft info, assume 1 (true for most)\n",
    "\n",
    "df = pd.merge(data,aircraft_counts,on='ev_id',how='left') # Concatenates \"aircraft count\" var to dataset, will indicate how many replicate rows to generate\n",
    "df['aircraft_count'] = df['aircraft_count'].fillna(1) # Some events were not in any of the 3 supplemental sets - set aircraft count as 1 for these\n",
    "\n",
    "df_repeated = df.loc[df.index.repeat(df['aircraft_count'])].copy() # Creates repeated rows based on # of aircraft (indicated in column we created)\n",
    "\n",
    "\n",
    "df_repeated['Aircraft_ID'] = df_repeated.groupby('ev_id').cumcount() + 1 # Re-creates aircraft ID in the event data so we can join the individual aircraft from the tables dataset\n",
    "df_repeated['event_key'] = df_repeated['ev_id'].astype(str) + '_' + df_repeated['Aircraft_ID'].astype(str) # Re-creates \"event_key\" for same reason.\n",
    "\n",
    "merged = df_repeated.merge(tables, on=['event_key','ev_id','Aircraft_ID'], how='left') # Joins \"event\" dataset with supplemental sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "348c2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../data/ntsb/cleaned/master.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d957fa",
   "metadata": {},
   "source": [
    "### Restricting Problem Scope\n",
    "\n",
    "We are only considering data from accidents that occurred in the United States due to the large amount of missing data in other rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b136ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.loc[(merged['ev_country']==\"USA\") & (merged['ev_type']==\"ACC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a61574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.loc[merged['damage'].isna(),'damage'] = 'UNK'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47031a2e",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "For now, we are taking our training set to be a random 80% sample of the pre-2020 flights. Note that in multi-aircraft crashes, we need to ensure that the aircraft do not get separated, i.e. some in the train set and some in the test set, as this would cause data leakage. We also want to stratify on damage. Hence the use of `StratifiedGroupKFold`. See [this stack overflow post](https://stackoverflow.com/questions/56872664/complex-dataset-split-stratifiedgroupshufflesplit) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c333851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de203d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_covid = merged.loc[merged['ev_year'] < 2020]\n",
    "post_covid = merged.loc[merged['ev_year'] >= 2020]\n",
    "\n",
    "pre_covid = pre_covid.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9291aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_kfold = StratifiedGroupKFold(n_splits = 5, shuffle=True, random_state=412)  # Validation and test will both be 20% of data, hence 5 splits\n",
    "\n",
    "splits = strat_kfold.split(pre_covid, pre_covid['damage'], pre_covid['ev_id'].astype(str))   # Grouped by 'ev_id', stratified by 'damage'\n",
    "\n",
    "# data_test <-- test set from the first fold (20% of data)\n",
    "# data_test <-- test set from the second fold (20% of data)\n",
    "# data_train <- remaining data (60%)\n",
    "for i, (train_index, test_index) in enumerate(splits):\n",
    "    if i==0:\n",
    "        data_test = pre_covid.iloc[test_index]\n",
    "        data_train = pre_covid.iloc[train_index]\n",
    "    elif i==1:\n",
    "        data_val = pre_covid.iloc[test_index]\n",
    "        data_train.drop(test_index)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b95b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('../data/ntsb/cleaned/master_train.csv', index=False)\n",
    "data_val.to_csv('../data/ntsb/cleaned/master_val.csv', index=False)\n",
    "data_test.to_csv('../data/ntsb/cleaned/master_test.csv', index=False)\n",
    "post_covid.to_csv('../data/ntsb/cleaned/master_post_covid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
