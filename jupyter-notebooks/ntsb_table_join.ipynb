{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a3cd7e9",
   "metadata": {},
   "source": [
    "# NTSB Table Join\n",
    "\n",
    "This script takes separate tables from the NTSB data (2008-2025) and joins them together while respecting the aircraft-level unit analysis found in the supplemental tables. To do this, we construct a \"event_key\" variable which can be understood as a measure of a single aircraft in an event of some nature, rather than a measure at the event-level itself.  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91f639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd4f35",
   "metadata": {},
   "source": [
    "### \"Engines\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c893b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engines = pd.read_csv('../data/ntsb/ntsb_engines.csv',usecols=['ev_id',\n",
    "                                                            'Aircraft_Key',\n",
    "                                                            'eng_type',\n",
    "                                                            'eng_no']) # Extract relevant columns\n",
    "\n",
    "engines = engines[engines['eng_no']==1].drop(columns='eng_no') # Record information about first engine only\n",
    "engines['event_key'] = engines['ev_id'].astype(str) + '_' + engines['Aircraft_Key'].astype(str) # Create event_key \n",
    "\n",
    "\n",
    "# engine_dummies = pd.get_dummies(data['eng_type'],dtype=int,prefix='engine') # Dummies of engine type\n",
    "\n",
    "# engines = pd.concat([data[['event_key','ev_id','Aircraft_Key']],engine_dummies],axis=1) # Combine \n",
    "\n",
    "#export.to_csv('../data/ntsb/cleaned/engine_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e1151",
   "metadata": {},
   "source": [
    "# \"Aircraft\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded177ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_15568/1860651589.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../data/ntsb/ntsb_aircraft.csv',usecols=['ev_id',\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/ntsb/ntsb_aircraft.csv',usecols=['ev_id',\n",
    "                                                             'Aircraft_Key',\n",
    "                                                             'far_part',\n",
    "                                                             'damage',\n",
    "                                                             'acft_fire',\n",
    "                                                             'acft_expl',\n",
    "                                                             'acft_make',\n",
    "                                                             'acft_model',\n",
    "                                                             'acft_category',\n",
    "                                                             'homebuilt',\n",
    "                                                             'total_seats',\n",
    "                                                             'num_eng',\n",
    "                                                             'fixed_retractable',\n",
    "                                                             'date_last_insp',\n",
    "                                                             'owner_acft',\n",
    "                                                             'certs_held',\n",
    "                                                             'oprtng_cert',\n",
    "                                                             'oper_cert',\n",
    "                                                             'type_fly',\n",
    "                                                             'second_pilot',\n",
    "                                                             'evacuation',\n",
    "                                                             'rwy_len',\n",
    "                                                             'rwy_width',\n",
    "                                                             'acft_year',\n",
    "                                                             'fuel_on_board',\n",
    "                                                             'unmanned']) # Extract Relevant Columns\n",
    "\n",
    "\n",
    "data['event_key'] = data['ev_id'].astype(str) + '_' + data['Aircraft_Key'].astype(str) # Event key \n",
    "aircraft = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90828e44",
   "metadata": {},
   "source": [
    "# \"Findings\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5404779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_15568/4107059561.py:1: DtypeWarning: Columns (0,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/ntsb/ntsb_findings.csv',usecols=['ev_id',\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ntsb/ntsb_findings.csv',usecols=['ev_id',\n",
    "                                                             'Aircraft_Key',\n",
    "                                                             'finding_description',\n",
    "                                                             'Cause_Factor']) # Extract Relevant Columns\n",
    "\n",
    "indexCorF = df[ (df['Cause_Factor'] != 'C') & (df['Cause_Factor'] != 'F') ].index # Remove obs. that are not causes or cause factors\n",
    "df.drop(indexCorF , inplace=True)\n",
    "\n",
    "df['event_key'] = df['ev_id'].astype(str) + '_' + df['Aircraft_Key'].astype(str) # Generate Event Key\n",
    "\n",
    "df = df[~df.finding_description.str.startswith('Not determined')] # Remove \"not determined\" cause factors\n",
    "\n",
    "df['finding_description_category'] = df.finding_description.str.split('-').str[0] # Separate broader CF category \n",
    "df.drop(df[df['finding_description_category'] == 'main system'].index, inplace = True) # Remove main system\n",
    "findings_dummies = pd.get_dummies(df['finding_description_category'],dtype=int) # Dummy out Categories \n",
    "new_df = pd.concat([df,findings_dummies],axis=1).drop(columns=['finding_description_category',\n",
    "                                                               'Cause_Factor',\n",
    "                                                               'ev_id',\n",
    "                                                               'Aircraft_Key'])\n",
    "new_df\n",
    "new_df = new_df.groupby(new_df['event_key']).sum()\n",
    "findings = new_df.reset_index()\n",
    "findings['Aircraft_Key']=findings[\"event_key\"].str[-1].astype(int)\n",
    "findings['ev_id'] = findings[\"event_key\"].str[:-2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a5ba298",
   "metadata": {},
   "source": [
    "# \"Injuries\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef5944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_15568/456845013.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/ntsb/ntsb_injuries.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NONE', 'SERS', 'FATL', 'MINR', 'TOTL']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/ntsb/ntsb_injuries.csv')\n",
    "\n",
    "## Remove NaN values to ensure valid summation when grouping by 'event_key'\n",
    "df = df[~df['inj_person_count'].isnull()]\n",
    "\n",
    "## Set new id for each case\n",
    "df['event_key'] = df['ev_id'].astype(str) + '_' + df['Aircraft_Key'].astype(str)\n",
    "\n",
    "## All possible injury levels\n",
    "print(list(set(df['injury_level'].values)))\n",
    "injury_levels = ['NONE', 'MINR', 'FATL', 'SERS']\n",
    "injured_levels = ['MINR', 'FATL', 'SERS']\n",
    "\n",
    "## Count the number of people at each injury level and the total number of injured individuals.\n",
    "\n",
    "injury_count_df = (\n",
    "    df[df['injury_level'].isin(injury_levels)]\n",
    "    .pivot_table(index='event_key', \n",
    "                 columns='injury_level', \n",
    "                 values='inj_person_count', \n",
    "                 aggfunc='sum', \n",
    "                 fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "## Rename columns\n",
    "injury_count_df.columns.name = None\n",
    "injury_count_df = injury_count_df.rename(columns={\n",
    "    'FATL': 'Fatal_count',\n",
    "    'SERS': 'Serious_count',\n",
    "    'MINR': 'Minor_count',\n",
    "    'NONE': 'None_count'\n",
    "})\n",
    "\n",
    "## Add total and injured counts\n",
    "injury_count_df['total_person_count'] = (\n",
    "    injury_count_df[['Fatal_count', 'Serious_count', 'Minor_count', 'None_count']].sum(axis=1)\n",
    ")\n",
    "injury_count_df['injured_person_count'] = (\n",
    "    injury_count_df[['Fatal_count', 'Serious_count', 'Minor_count']].sum(axis=1)\n",
    ")\n",
    "\n",
    "injury_count_df['ev_id'] = injury_count_df['event_key'].str.split('_').str[0]\n",
    "injury_count_df['Aircraft_Key'] = injury_count_df['event_key'].str.split('_').str[1].astype(int)\n",
    "injuries = injury_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08107005",
   "metadata": {},
   "source": [
    "# \"Event\" Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80b90e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/40/pdcq1fy521j_fy5myrnctghc0000gn/T/ipykernel_15568/699352926.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('../data/ntsb/ntsb_events.csv',usecols=['ev_id',\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../data/ntsb/ntsb_events.csv',usecols=['ev_id',\n",
    "                                                            'ntsb_no', # for exploratory purposes; makes it easier to find docket on NTSB website\n",
    "                                                            'ev_country',\n",
    "                                                            'ev_type',\n",
    "                                                            'ev_highest_injury',\n",
    "                                                            'inj_f_grnd',\n",
    "                                                            'inj_m_grnd',\n",
    "                                                            'inj_s_grnd',\n",
    "                                                            'inj_tot_f',\n",
    "                                                            'inj_tot_m',\n",
    "                                                            'inj_tot_m',\n",
    "                                                            'inj_tot_n',\n",
    "                                                            'inj_tot_s',\n",
    "                                                            'inj_tot_t',\n",
    "                                                            'ev_time',\n",
    "                                                            'ev_year',\n",
    "                                                            'ev_month',\n",
    "                                                            'on_ground_collision',\n",
    "                                                            'latitude',\n",
    "                                                            'longitude',\n",
    "                                                            'apt_dist',\n",
    "                                                            'light_cond',\n",
    "                                                            'wx_dew_pt',\n",
    "                                                            'wind_vel_kts',\n",
    "                                                            'gust_kts',\n",
    "                                                            'altimeter',\n",
    "                                                            ]) # Select variables - this is our primary \"data\" object to which we will concatenate all  other sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fb5d58",
   "metadata": {},
   "source": [
    "# Joining Event Data with Aircraft-Specific Data\n",
    "To join the primary \"event\" dataset, which is event-specific with the other tables which are aircraft-specific, we need to take a careful approach to how we go about joining them.  The general idea goes as follows:\n",
    "1. Create the \"event_key\" variable in the aircraft-specific datasets, which takes the format '{ev_id}_{Aircraft_Key}'.\n",
    "2. Join the lower-level datasets together to maximize the number of observations.  Some will likely be in one and not another, but what is important is that we collect a list of all individual aircraft-level observations.\n",
    "3. Once all lower-level datasets have been joined together and we have a list of all events with multiple aircraft, we can export a \"aircraft_count\" variable which expresses the number of \"Aircraft_Key\" for every \"ev_id.\"  \n",
    "4. Join this \"aircraft_count\" column into the \"Event\" dataset - now we have a count of how many planes were involved in each event.\n",
    "5. Create a function which duplicates every row in \"Events\" (aircraft_count - 1 times).  Thus, if there's 3 planes, we'll get 2 new rows of the event.\n",
    "6. Re-create the \"Aircraft\" variable with a groupby() and cum_count() function, so that every row per ev_id is added to until there are no more observations left (will be clearer in the code).\n",
    "7. Now that we have the dataset formatted to resemble the individual-aircraft-level data from other tables, we can create the \"event_key\" - our master joining variable - in the events data.\n",
    "8. Join all datasets on the Events data by \"event_key\", \"Aircraft_Key\", and \"ev_id\" to ensure we are joining the right aircraft/event combos onto the event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099e17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.merge(engines,aircraft,on=['event_key','ev_id','Aircraft_Key'],how='left') # Join on all 3 to avoid _x and _y duplicate columns.  Also ensures correct specification.\n",
    "tables = pd.merge(tables,findings,on=['event_key','ev_id','Aircraft_Key'],how='left') # Join \"findings\" data as well.\n",
    "tables = pd.merge(tables,injuries,on=['event_key','ev_id','Aircraft_Key'],how='left') # Join \"injuries\" data as well.\n",
    "tables['Aircraft_ID'] = tables.groupby('ev_id').cumcount() + 1 # Some events start at 2 - this line creates a new Aircraft Key that is uniform across all coding schemes.\n",
    "tables['event_key'] = tables['ev_id'].astype(str) + '_' + tables['Aircraft_ID'].astype(str) # Resets the \"event_key\" variable to match our adjusted aircraft ID\n",
    "\n",
    "aircraft_counts = pd.DataFrame(tables.groupby('ev_id')['Aircraft_ID'].count()).reset_index() # Counts how many unique values of \"Aircraft_ID\" per event - need this to tell \"events\" set where to duplicate\n",
    "aircraft_counts.rename(columns={'Aircraft_ID':'aircraft_count'},inplace=True)\n",
    "aircraft_counts['aircraft_count'] = aircraft_counts['aircraft_count'].fillna(1) # Fill in missing - if no aircraft info, assume 1 (true for most)\n",
    "\n",
    "df = pd.merge(data,aircraft_counts,on='ev_id',how='left') # Concatenates \"aircraft count\" var to dataset, will indicate how many replicate rows to generate\n",
    "df['aircraft_count'] = df['aircraft_count'].fillna(1) # Some events were not in any of the 3 supplemental sets - set aircraft count as 1 for these\n",
    "\n",
    "df_repeated = df.loc[df.index.repeat(df['aircraft_count'])].copy() # Creates repeated rows based on # of aircraft (indicated in column we created)\n",
    "\n",
    "\n",
    "df_repeated['Aircraft_ID'] = df_repeated.groupby('ev_id').cumcount() + 1 # Re-creates aircraft ID in the event data so we can join the individual aircraft from the tables dataset\n",
    "df_repeated['event_key'] = df_repeated['ev_id'].astype(str) + '_' + df_repeated['Aircraft_ID'].astype(str) # Re-creates \"event_key\" for same reason.\n",
    "\n",
    "merged = df_repeated.merge(tables, on=['event_key','ev_id','Aircraft_ID'], how='left') # Joins \"event\" dataset with supplemental sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348c2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../data/ntsb/cleaned/master.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f565c8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ev_id</th>\n",
       "      <th>ntsb_no</th>\n",
       "      <th>ev_type</th>\n",
       "      <th>ev_time</th>\n",
       "      <th>ev_country</th>\n",
       "      <th>ev_year</th>\n",
       "      <th>ev_month</th>\n",
       "      <th>on_ground_collision</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Aircraft</th>\n",
       "      <th>Environmental issues</th>\n",
       "      <th>Organizational issues</th>\n",
       "      <th>Personnel issues</th>\n",
       "      <th>Fatal_count</th>\n",
       "      <th>Minor_count</th>\n",
       "      <th>None_count</th>\n",
       "      <th>Serious_count</th>\n",
       "      <th>total_person_count</th>\n",
       "      <th>injured_person_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20080211X00175</td>\n",
       "      <td>DFW08RA039</td>\n",
       "      <td>ACC</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>049130N</td>\n",
       "      <td>0122412W</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20080107X00026</td>\n",
       "      <td>SEA08LA057</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>381326N</td>\n",
       "      <td>1222659W</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20080107X00026</td>\n",
       "      <td>SEA08LA057</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2230.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>381326N</td>\n",
       "      <td>1222659W</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20080109X00036</td>\n",
       "      <td>DFW08CA054</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293022N</td>\n",
       "      <td>0952836W</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080107X00027</td>\n",
       "      <td>DFW08LA055</td>\n",
       "      <td>ACC</td>\n",
       "      <td>825.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>035321N</td>\n",
       "      <td>0973829W</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29067</th>\n",
       "      <td>20250425200067</td>\n",
       "      <td>ERA25LA186</td>\n",
       "      <td>ACC</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>385629N</td>\n",
       "      <td>0081574W</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29068</th>\n",
       "      <td>20250426200068</td>\n",
       "      <td>GAA25WA145</td>\n",
       "      <td>ACC</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>BN</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29069</th>\n",
       "      <td>20250429200075</td>\n",
       "      <td>GAA25WA147</td>\n",
       "      <td>ACC</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>CO</td>\n",
       "      <td>2025</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29070</th>\n",
       "      <td>20250429200077</td>\n",
       "      <td>WPR25LA143</td>\n",
       "      <td>ACC</td>\n",
       "      <td>1305.0</td>\n",
       "      <td>USA</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29071</th>\n",
       "      <td>20250430200082</td>\n",
       "      <td>GAA25WA148</td>\n",
       "      <td>ACC</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>PL</td>\n",
       "      <td>2025</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29072 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ev_id     ntsb_no ev_type  ev_time ev_country  ev_year  \\\n",
       "0      20080211X00175  DFW08RA039     ACC   1907.0         CA     2008   \n",
       "1      20080107X00026  SEA08LA057     ACC   2230.0        USA     2008   \n",
       "2      20080107X00026  SEA08LA057     ACC   2230.0        USA     2008   \n",
       "3      20080109X00036  DFW08CA054     ACC   2200.0        USA     2008   \n",
       "4      20080107X00027  DFW08LA055     ACC    825.0        USA     2008   \n",
       "...               ...         ...     ...      ...        ...      ...   \n",
       "29067  20250425200067  ERA25LA186     ACC   2022.0        USA     2025   \n",
       "29068  20250426200068  GAA25WA145     ACC   1440.0         BN     2025   \n",
       "29069  20250429200075  GAA25WA147     ACC   1420.0         CO     2025   \n",
       "29070  20250429200077  WPR25LA143     ACC   1305.0        USA     2025   \n",
       "29071  20250430200082  GAA25WA148     ACC   1843.0         PL     2025   \n",
       "\n",
       "       ev_month on_ground_collision latitude longitude  ...  Aircraft  \\\n",
       "0             1                 NaN  049130N  0122412W  ...       NaN   \n",
       "1             1                   N  381326N  1222659W  ...       0.0   \n",
       "2             1                   N  381326N  1222659W  ...       0.0   \n",
       "3             1                 NaN  293022N  0952836W  ...       1.0   \n",
       "4             1                 NaN  035321N  0973829W  ...       0.0   \n",
       "...         ...                 ...      ...       ...  ...       ...   \n",
       "29067         4                 NaN  385629N  0081574W  ...       NaN   \n",
       "29068         1                 NaN      NaN       NaN  ...       NaN   \n",
       "29069         2                 NaN      NaN       NaN  ...       NaN   \n",
       "29070         4                 NaN      NaN       NaN  ...       NaN   \n",
       "29071         4                 NaN      NaN       NaN  ...       NaN   \n",
       "\n",
       "      Environmental issues  Organizational issues  Personnel issues  \\\n",
       "0                      NaN                    NaN               NaN   \n",
       "1                      0.0                    0.0               1.0   \n",
       "2                      0.0                    0.0               1.0   \n",
       "3                      1.0                    0.0               0.0   \n",
       "4                      1.0                    0.0               1.0   \n",
       "...                    ...                    ...               ...   \n",
       "29067                  NaN                    NaN               NaN   \n",
       "29068                  NaN                    NaN               NaN   \n",
       "29069                  NaN                    NaN               NaN   \n",
       "29070                  NaN                    NaN               NaN   \n",
       "29071                  NaN                    NaN               NaN   \n",
       "\n",
       "       Fatal_count  Minor_count None_count  Serious_count  total_person_count  \\\n",
       "0              NaN          NaN        NaN            NaN                 NaN   \n",
       "1              0.0          0.0        1.0            0.0                 1.0   \n",
       "2              0.0          0.0        2.0            0.0                 2.0   \n",
       "3              0.0          0.0        1.0            0.0                 1.0   \n",
       "4              0.0          0.0        8.0            0.0                 8.0   \n",
       "...            ...          ...        ...            ...                 ...   \n",
       "29067          NaN          NaN        NaN            NaN                 NaN   \n",
       "29068          NaN          NaN        NaN            NaN                 NaN   \n",
       "29069          NaN          NaN        NaN            NaN                 NaN   \n",
       "29070          NaN          NaN        NaN            NaN                 NaN   \n",
       "29071          NaN          NaN        NaN            NaN                 NaN   \n",
       "\n",
       "       injured_person_count  \n",
       "0                       NaN  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       0.0  \n",
       "4                       0.0  \n",
       "...                     ...  \n",
       "29067                   NaN  \n",
       "29068                   NaN  \n",
       "29069                   NaN  \n",
       "29070                   NaN  \n",
       "29071                   NaN  \n",
       "\n",
       "[29072 rows x 65 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47031a2e",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "For now, we are taking our training set to be a random 80% sample of the pre-2020 flights. Note that in multi-aircraft crashes, we need to ensure that the aircraft do not get separated, i.e. some in the train set and some in the test set, as this would cause data leakage. Hence the use of `GroupShuffleSplit`. See [this stack overflow post](https://stackoverflow.com/questions/54797508/how-to-generate-a-train-test-split-based-on-a-group-id) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c333851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de203d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_covid = merged.loc[merged['ev_year'] < 2020]\n",
    "post_covid = merged.loc[merged['ev_year'] >= 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6bc789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split so that rows with the same ev_id end up in the same set\n",
    "splitter = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 412)\n",
    "split = splitter.split(pre_covid, groups=pre_covid['ev_id'].astype(str))    # Necessary to cast ev_id as strings because some are ints and split() does internal sorting\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "data_train = pre_covid.iloc[train_inds]\n",
    "data_test = pre_covid.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b95b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('../data/ntsb/cleaned/master_train.csv', index=False)\n",
    "data_test.to_csv('../data/ntsb/cleaned/master_test.csv', index=False)\n",
    "post_covid.to_csv('../data/ntsb/cleaned/master_post_covid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363e137",
   "metadata": {},
   "source": [
    "# Considerations Going Forward\n",
    "- Do we drop observations where Aircraft_Key (from original tables) is missing? If all aircraft info is missing, in order words, do we just drop the event?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
